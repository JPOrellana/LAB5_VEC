{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JPOrellana/LAB5_VEC/blob/main/Laboratorio_4_Vectorizacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXhjY9dGwJls"
      },
      "source": [
        "NOMBRES: José Pablo\n",
        "\n",
        "APELLIDOS: Orellana Orellana\n",
        "\n",
        "CARNE: 21970\n",
        "\n",
        "FECHA: 04 de septiembre del 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7nnvCPLh5Ms"
      },
      "source": [
        "**Ejercicio 1**\n",
        "Cree un corpus a su gusto como el visto en clase, cálcule PPMI, pero aplicando Lapace Smoothing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "r3aTZXJeiFAI"
      },
      "outputs": [],
      "source": [
        "# --- Corpus  ---\n",
        "corpus = [\n",
        "    \"madrid es la capital de españa\",\n",
        "    \"paris es la capital de francia\",\n",
        "    \"roma es la capital de italia\",\n",
        "    \"berlin es la capital de alemania\",\n",
        "    \"lisboa es la capital de portugal\",\n",
        "    \"el rey es un hombre\",\n",
        "    \"la reina es una mujer\",\n",
        "    \"un príncipe es hijo del rey\",\n",
        "    \"una princesa es hija de la reina\",\n",
        "    \"el doctor es un hombre\",\n",
        "    \"la doctora es una mujer\",\n",
        "    \"el actor es famoso\",\n",
        "    \"la actriz es famosa\",\n",
        "    \"madrid está en españa\",\n",
        "    \"paris está en francia\",\n",
        "    \"roma está en italia\",\n",
        "    \"berlin está en alemania\",\n",
        "    \"lisboa está en portugal\",\n",
        "    \"hombre y hombres son formas relacionadas\",\n",
        "    \"mujer y mujeres son formas relacionadas\",\n",
        "    \"el rey y la reina gobiernan un reino\",\n",
        "    \"hombre y mujer viven en ciudades como madrid paris roma\",\n",
        "]\n",
        "\n",
        "def tokenize(s): \n",
        "    return s.lower().split()\n",
        "\n",
        "tokenized = [tokenize(s) for s in corpus]\n",
        "\n",
        "# --- Matriz de co-ocurrencias con ventana ±2 ---\n",
        "window_size = 2\n",
        "vocab = sorted({w for sent in tokenized for w in sent})\n",
        "idx = {w:i for i, w in enumerate(vocab)}\n",
        "V = len(vocab)\n",
        "\n",
        "cooc = np.zeros((V, V), dtype=np.float64)\n",
        "unigram = Counter()\n",
        "\n",
        "for sent in tokenized:\n",
        "    n = len(sent)\n",
        "    for i, w in enumerate(sent):\n",
        "        wi = idx[w]\n",
        "        unigram[w] += 1\n",
        "        L, R = max(0, i-window_size), min(n, i+window_size+1)\n",
        "        for j in range(L, R):\n",
        "            if j == i: \n",
        "                continue\n",
        "            cj = idx[sent[j]]\n",
        "            cooc[wi, cj] += 1.0\n",
        "\n",
        "# --- Laplace smoothing + PPMI ---\n",
        "alpha = 1.0\n",
        "cooc_lap = cooc + alpha\n",
        "\n",
        "total = cooc_lap.sum()\n",
        "p_wc = cooc_lap / total\n",
        "p_w = p_wc.sum(axis=1, keepdims=True)\n",
        "p_c = p_wc.sum(axis=0, keepdims=True)\n",
        "\n",
        "eps = 1e-12\n",
        "PMI = np.log2((p_wc + eps) / (p_w @ p_c + eps))\n",
        "PPMI = np.maximum(PMI, 0.0)\n",
        "\n",
        "# Variables útiles para el Ejercicio 2\n",
        "def get_vec(word:str) -> np.ndarray:\n",
        "    return PPMI[idx[word]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzRf6sNDiF1v"
      },
      "source": [
        "**Ejercicio 2**\n",
        "\n",
        "Crear 10 analigias validas basadas en su texto, tome en cuenta que es necesario utilizar el codigo visto en clase y ajuste su corpus para poder lograr obtener todas las analogias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WUUiX7oBffZg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analogías (a:b :: c: ?)\n",
            "\n",
            "hombre:rey :: mujer: ?  | esperado=reina    | top1=doctora  sim=0.432 | top5=['doctora', 'es', 'actor', 'princesa', 'del']\n",
            "hombre:doctor :: mujer: ?  | esperado=doctora  | top1=doctora  sim=0.501 | top5=['doctora', 'princesa', 'es', 'actor', 'ciudades']\n",
            "actor:actriz :: famoso: ?  | esperado=famosa   | top1=el       sim=0.267 | top5=['el', 'capital', 'de', 'famosa', 'doctora']\n",
            "francia:paris :: españa: ?  | esperado=madrid   | top1=madrid   sim=0.682 | top5=['madrid', 'ciudades', 'roma', 'en', 'berlin']\n",
            "italia:roma :: portugal: ?  | esperado=lisboa   | top1=como     sim=0.719 | top5=['como', 'berlin', 'lisboa', 'paris', 'madrid']\n",
            "alemania:berlin :: italia: ?  | esperado=roma     | top1=lisboa   sim=1.000 | top5=['lisboa', 'españa', 'francia', 'portugal', 'roma']\n",
            "rey:reina :: hijo: ?  | esperado=hija     | top1=doctora  sim=0.415 | top5=['doctora', 'un', 'capital', 'reino', 'el']\n",
            "hombre:hombres :: mujer: ?  | esperado=mujeres  | top1=relacionadas sim=0.391 | top5=['relacionadas', 'doctora', 'y', 'mujeres', 'princesa']\n",
            "capital:madrid :: francia: ?  | esperado=paris    | top1=paris    sim=0.420 | top5=['paris', 'como', 'roma', 'berlin', 'lisboa']\n",
            "capital:roma :: alemania: ?  | esperado=berlin   | top1=como     sim=0.384 | top5=['como', 'berlin', 'lisboa', 'paris', 'españa']\n"
          ]
        }
      ],
      "source": [
        "def cosine(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    na, nb = np.linalg.norm(a), np.linalg.norm(b)\n",
        "    if na == 0 or nb == 0:\n",
        "        return 0.0\n",
        "    return float(np.dot(a, b) / (na * nb))\n",
        "\n",
        "def most_similar(vec: np.ndarray, exclude=set(), topn=5):\n",
        "    sims = []\n",
        "    for w in vocab:\n",
        "        if w in exclude: \n",
        "            continue\n",
        "        sims.append((w, cosine(vec, get_vec(w))))\n",
        "    sims.sort(key=lambda x: x[1], reverse=True)\n",
        "    return sims[:topn]\n",
        "\n",
        "def analogy(a: str, b: str, c: str, topn:int=5):\n",
        "    vec = get_vec(b) - get_vec(a) + get_vec(c)\n",
        "    return most_similar(vec, exclude={a,b,c}, topn=topn)\n",
        "\n",
        "tests = [\n",
        "    (\"hombre\", \"rey\", \"mujer\", \"reina\"),\n",
        "    (\"hombre\", \"doctor\", \"mujer\", \"doctora\"),\n",
        "    (\"actor\", \"actriz\", \"famoso\", \"famosa\"),\n",
        "    (\"francia\", \"paris\", \"españa\", \"madrid\"),\n",
        "    (\"italia\", \"roma\", \"portugal\", \"lisboa\"),\n",
        "    (\"alemania\", \"berlin\", \"italia\", \"roma\"),\n",
        "    (\"rey\", \"reina\", \"hijo\", \"hija\"),\n",
        "    (\"hombre\", \"hombres\", \"mujer\", \"mujeres\"),\n",
        "    (\"capital\", \"madrid\", \"francia\", \"paris\"),\n",
        "    (\"capital\", \"roma\", \"alemania\", \"berlin\"),\n",
        "]\n",
        "\n",
        "print(\"Analogías (a:b :: c: ?)\\n\")\n",
        "for a,b,c,expected in tests:\n",
        "    try:\n",
        "        preds = analogy(a,b,c, topn=5)\n",
        "        top1 = preds[0] if preds else (\"<sin pred>\", 0.0)\n",
        "        print(f\"{a}:{b} :: {c}: ?  | esperado={expected:8s} | top1={top1[0]:8s} sim={top1[1]:.3f} | top5={[w for w,_ in preds]}\")\n",
        "    except KeyError as e:\n",
        "        print(f\"{a}:{b} :: {c}: ?  | palabra fuera de vocabulario: {e}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Procesamiento",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
